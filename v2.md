## MEAD 2.0 Release

### Goals for Version 2

- Support TF eager completely
  - TODO: figuring out a good/fast solution for s2s/LM
- Better support for tf.dataset and tf.estimator
  - Mostly done, some multi-worker issues with estimator code
- Better a la carte support (providing layers that work outside of mead)
  - TODO: Promote up optimizer utilities with same interface?
- Underlying layers API that is identical between PyTorch and TF
  - Some cases pytorch requires explicit dims and TF no longer does which makes things more clunky than they need to be
- Get rid of python 2, make typing more explicit
- Improved documentation than previous offering
- Simplify by removing `dynet` support and non TF `keras` support
- Simplify services and provide a more straightforward `predict` like functionality with an array

#### Open Items

- Sparse update ops dont work on GPU, In TF 2 this leads to errors, particularly with SGD w/ Momentum and Adadelta.  Supposedly this will be fixed in TF in November
  - https://github.com/tensorflow/tensorflow/issues/31291
  - This issues causes sub-optimal results in tagger configurations particularly, for which we have tuned the HPs specifically with momentum.  For this reason, we do not recommend training taggers with default HPs with TF2
  - Currently, if a user requests either Adadelta or SGD w/ Momentum, a warning is emitted and the optimizer is switched underneath
    - In the case of Momentum, it is removed :(
    - In the case of Adadelta, Adam is applied instead with `min(requested_lr, 0.001)`

- There is no way to run multi-GPU in TF with early stopping.  This is because we use estimators with `train_and_evaluate`, which doesnt support early stopping

- Not all estimator implementations properly record metrics so that they can be used by `xpctl` (this will hopefully be fixed before release)

- Baseline model save and export in eager mode is broken (this will be fixed before release)

