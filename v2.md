## MEAD 2.0 Release

### Goals for Version 2

- Support TF eager completely
  - TODO: figuring out a good/fast solution for s2s/LM
- Better support for native Datasets
  - For TensorFlow, tf.dataset and tf.estimator
    - Mostly done, some multi-worker issues with estimator code
  - For PyTorch, use DataLoader
- Better a la carte support (providing layers that work outside of mead)
- Underlying layers API that is nearly identical between PyTorch and TF
  - Some cases pytorch requires explicit dims and TF no longer does which makes things more clunky than they need to be
- Get rid of python 2, make typing more explicit
- Improved documentation than previous offering
- Simplify by removing `dynet` support and non TF `keras` support
- Simplify services and provide a more straightforward `predict` like functionality with an array
- Updated API examples for accessing layers directly, including large-scale Transformer pre-training code with fine-tuning addons
- Comprehensive support for BERT and BPE-based Transformer models in TF and PyTorch
- Support for [mead-hub](https://github.com/mead-ml/hub), a centralized repository for sharing models, tasks, vectorizers and embeddings
  - [ELMo example](https://github.com/mead-ml/hub/blob/master/v1/addons/embed_elmo_tf.py) - by specifying this addon in your [mead config](https://github.com/dpressel/mead-baseline/blob/feature/v2/mead/config/sst2-elmo-eh.json)

#### Open Items

- Sparse update ops dont work on GPU, In TF 2 this leads to errors, particularly with SGD w/ Momentum and Adadelta.  Supposedly this will be fixed in TF in November
  - https://github.com/tensorflow/tensorflow/issues/31291
  - This issues causes sub-optimal results in tagger configurations particularly, for which we have tuned the HPs specifically with momentum.  For this reason, we do not recommend training taggers with default HPs with TF2
  - Currently, if a user requests either Adadelta or SGD w/ Momentum, a warning is emitted and the optimizer is switched underneath
    - In the case of Momentum, it is removed :(
    - In the case of Adadelta, Adam is applied instead with `min(requested_lr, 0.001)`

- There is no way to run multi-GPU in TF with early stopping.  This is because we use estimators with `train_and_evaluate`, which doesnt support early stopping

- Not all estimator implementations properly record metrics so that they can be used by `xpctl` (this will hopefully be fixed before release)

- Baseline model save and export in eager mode is broken (this will be fixed before release)

