backend: pytorch
basedir: ./sst2-bert-base-uncased
batchsz: 32
dataset: SST2
features:
- embeddings:
    word_embed_type: learned-positional
    label: gpt2-npz
    type: tlm-words-embed-pooled
    pooling: mean
    transformer_type: pre-layer-norm
    layer_norms_after: false
    layer_norm_eps: 1.0e-5
    activation: gelu_new
    finetune: true
    dropout: 0.1
    mlm: false
  name: gpt2
  vectorizer:
    mxlen: 100
    label: gpt2-bpe1d
loader:
  reader_type: default
model:
  model_type: fine-tune
task: classify
train:
  early_stopping_metric: acc
  epochs: 10
  eta: 1.0e-5
  optim: adamw
  weight_decay: 1.0e-5
  lr_scheduler_type: cosine
  decay_steps: 48100
unif: 0.1
